# RAMESH v2.0 - Technical Documentation ğŸ“šğŸ”¬

> **Complete Developer Guide for the AI-Powered Data Collector Agent**  
> Created by Sajak ğŸ‡³ğŸ‡µ | Last Updated: February 2026

---

## Table of Contents

1. [System Overview](#1-system-overview)
2. [Architecture](#2-architecture)
3. [File Structure](#3-file-structure)
4. [Core Components](#4-core-components)
5. [Data Flow](#5-data-flow)
6. [Configuration](#6-configuration)
7. [Database Schema](#7-database-schema)
8. [API Integrations](#8-api-integrations)
9. [Running the System](#9-running-the-system)
10. [Troubleshooting](#10-troubleshooting)

---

## 1. System Overview

### What is RAMESH?

RAMESH (named with Nepali humor ğŸ‡³ğŸ‡µ) is an AI-powered data collection agent designed to build large-scale training datasets for LLMs. It supports two data sources:

| Source | Type | Limit | Authentication |
|--------|------|-------|----------------|
| **Z-Library** | Books (PDFs) | 9 downloads/account | Cookie-based |
| **arXiv** | Research Papers (PDFs) | Unlimited | None required |

### Key Features

- ğŸ¤– **AI Agent Interface**: Conversational UI powered by GPT-4
- â˜ï¸ **Cloud Memory**: Supabase-based shared duplicate detection
- ğŸ”„ **Multi-Account Rotation**: Automatic account switching for Z-Library
- ğŸ“Š **Bulk Collection**: Download thousands of papers with curated queries
- ğŸ’¾ **Checkpoint/Resume**: Interrupt and resume large collections
- ğŸ”’ **SSL Support**: Windows-compatible SSL certificate handling

---

## 2. Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RAMESH v2.0 ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚    â”‚   User CLI   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  agent.py    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   OpenAI     â”‚      â”‚
â”‚    â”‚  Interface   â”‚         â”‚ (AI Agent)   â”‚         â”‚   GPT-4o     â”‚      â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                    â”‚                                        â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                    â”‚               â”‚               â”‚                        â”‚
â”‚                    â–¼               â–¼               â–¼                        â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â”‚ mcp_server.pyâ”‚ â”‚arxiv_collectorâ”‚ â”‚  memory.py   â”‚               â”‚
â”‚           â”‚ (Z-Library)  â”‚ â”‚   (arXiv)    â”‚ â”‚  (Supabase)  â”‚               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                  â”‚                â”‚                â”‚                        â”‚
â”‚                  â–¼                â–¼                â–¼                        â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â”‚  Z-Library   â”‚ â”‚  arXiv API   â”‚ â”‚   Supabase   â”‚               â”‚
â”‚           â”‚   Website    â”‚ â”‚export.arxiv  â”‚ â”‚  PostgreSQL  â”‚               â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              DATA STORAGE

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                    â”‚
    â”‚   ğŸ“ data/books/     â†’ Z-Library downloaded PDFs                   â”‚
    â”‚   ğŸ“ data/papers/    â†’ arXiv downloaded PDFs                       â”‚
    â”‚   ğŸ“„ resources.json  â†’ Book metadata                               â”‚
    â”‚   ğŸ“„ arxiv_resources.json â†’ Paper metadata                         â”‚
    â”‚                                                                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

| Component | Responsibility |
|-----------|---------------|
| `agent.py` | Main AI agent, tool execution, conversation management |
| `mcp_server.py` | Z-Library scraping, book downloads, account rotation |
| `arxiv_collector.py` | arXiv API integration, paper downloads, rate limiting |
| `memory.py` | Supabase client, duplicate detection, embeddings |
| `curated_papers.py` | Predefined topic list for bulk arXiv collection |
| `run_curated_collection.py` | Automated bulk collection runner |

---

## 3. File Structure

```
DataCollectornValidatorAgent/
â”‚
â”œâ”€â”€ ğŸ“„ agent.py                    # Main AI agent (GPT-4o powered)
â”œâ”€â”€ ğŸ“„ mcp_server.py               # Z-Library downloader
â”œâ”€â”€ ğŸ“„ arxiv_collector.py          # arXiv paper collector
â”œâ”€â”€ ğŸ“„ memory.py                   # Supabase cloud memory
â”œâ”€â”€ ğŸ“„ curated_papers.py           # Curated topic definitions
â”œâ”€â”€ ğŸ“„ run_curated_collection.py   # Bulk collection runner
â”œâ”€â”€ ğŸ“„ run.py                      # Simple entry point
â”‚
â”œâ”€â”€ ğŸ“„ .env                        # Configuration (secrets)
â”œâ”€â”€ ğŸ“„ .env.example                # Configuration template
â”œâ”€â”€ ğŸ“„ accounts.json               # Z-Library account cookies
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“„ collection_checkpoint.json  # Resume checkpoint (auto-generated)
â”œâ”€â”€ ğŸ“„ collection_report.json      # Collection statistics (auto-generated)
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ ğŸ“ books/                  # Downloaded Z-Library PDFs
â”‚   â”œâ”€â”€ ğŸ“ papers/                 # Downloaded arXiv PDFs
â”‚   â”œâ”€â”€ ğŸ“„ resources.json          # Book metadata
â”‚   â””â”€â”€ ğŸ“„ arxiv_resources.json    # Paper metadata
â”‚
â””â”€â”€ ğŸ“„ new_technical_docs.md       # This documentation
```

---

## 4. Core Components

### 4.1 agent.py - The AI Brain

The main conversational agent that orchestrates all operations.

**Key Classes:**
```python
class DataSource(Enum):
    ZLIBRARY = "zlibrary"
    ARXIV = "arxiv"

class LibrarianAgent:
    def __init__(self, source: DataSource = DataSource.ZLIBRARY)
    async def execute_tool(self, tool_name, arguments) -> str
    async def chat(self, user_message: str) -> str
    def switch_source(self, new_source: DataSource)
```

**Flow:**
1. User starts agent â†’ selects data source (Z-Library or arXiv)
2. User sends message â†’ agent processes with GPT-4o
3. GPT-4o may call tools â†’ agent executes tools
4. Results returned â†’ agent responds to user

**Tools Available:**

| Tool | Source | Description |
|------|--------|-------------|
| `download_books` | Z-Library | Search and download books |
| `check_remaining_downloads` | Z-Library | Check account quotas |
| `search_memory` | Both | Search for existing downloads |
| `get_stats` | Both | Get collection statistics |
| `download_papers` | arXiv | Search and download papers |
| `list_arxiv_categories` | arXiv | Show arXiv category codes |
| `search_paper_memory` | arXiv | Search paper database |

---

### 4.2 arxiv_collector.py - Paper Downloader

Handles all arXiv API interactions and PDF downloads.

**Key Classes:**
```python
@dataclass
class ArxivPaper:
    arxiv_id: str          # e.g., "2301.00001"
    title: str
    authors: List[str]
    abstract: str
    categories: List[str]  # e.g., ["cs.LG", "cs.AI"]
    primary_category: str
    published: str
    updated: str
    pdf_url: str
    abs_url: str

class ArxivCollector:
    async def search_papers(query, categories, max_results) -> List[ArxivPaper]
    async def download_pdf(paper: ArxivPaper) -> Optional[str]
    async def collect_papers(query, categories, max_papers) -> dict
```

**Search Query Building:**
```python
# OLD (exact phrase - few results):
search_query = f'all:"{query}"'

# NEW (word-by-word AND - many more results):
words = query.split()
search_query = " AND ".join([f"all:{word}" for word in words])
# Example: "deep learning" â†’ (all:deep AND all:learning)
```

**Rate Limiting:**
- API requests: 3 seconds between calls
- PDF downloads: 1 second between files
- Automatic retry with exponential backoff

**SSL Fix for Windows:**
```python
import certifi
SSL_CONTEXT = ssl.create_default_context(cafile=certifi.where())
connector = aiohttp.TCPConnector(ssl=SSL_CONTEXT)
```

---

### 4.3 memory.py - Cloud Memory System

Supabase-based shared memory for multi-user duplicate detection.

**Why Supabase?**
- Multiple users need to share the same database
- SQLite is local-only, can't sync across PCs
- Supabase = Free PostgreSQL in the cloud

**Key Functions:**
```python
class AgentMemory:
    # Books (Z-Library)
    def check_duplicate(title, authors) -> dict
    def add_book(title, authors, source, topic, user) -> bool
    def get_stats() -> dict
    def search_similar(query, limit) -> list
    
    # Papers (arXiv)
    def check_paper_duplicate(arxiv_id, title) -> dict
    def add_paper(arxiv_id, title, authors, abstract, categories, user) -> bool
    def get_paper_stats() -> dict
    def search_papers_similar(query, limit) -> list
    
    # Combined
    def get_combined_stats() -> dict
```

**Duplicate Detection:**
1. First check exact match (arxiv_id or normalized_title)
2. If no exact match, compute embedding similarity
3. Similarity > 85% = duplicate

**Embedding Model:** `text-embedding-3-small` (OpenAI)

---

### 4.4 curated_papers.py - Topic Definitions

Predefined topics for quality-focused bulk collection.

**Structure:**
```python
CURATED_TOPICS = {
    "foundations": {
        "Neural Network Fundamentals": {
            "queries": [
                "backpropagation neural network learning",
                "universal approximation theorem neural",
                "deep learning representation learning"
            ],
            "papers_per_query": 10,
            "total_target": 30,
            "categories": ["cs.LG", "cs.NE"],
            "year_range": (2010, 2024)
        },
        # ... more topics
    },
    "architectures": { ... },
    "llm_nlp": { ... },
    "vision_multimodal": { ... },
    "training_efficiency": { ... },
    "evaluation_safety": { ... }
}

SURVEY_QUERIES = [
    {"query": "survey deep learning neural network", "max_papers": 20, "categories": ["cs.LG"]},
    # ... 9 more survey queries
]
```

**Categories (23 total):**
- Foundations: 5 topics (150 papers)
- Architectures: 6 topics (180 papers)
- LLM & NLP: 7 topics (205 papers)
- Vision & Multimodal: 4 topics (100 papers)
- Training Efficiency: 4 topics (100 papers)
- Evaluation & Safety: 2 topics (50 papers)
- Surveys: 10 queries (145 papers)

**Total Target: ~930 high-quality papers**

---

### 4.5 run_curated_collection.py - Bulk Runner

Automated collection with checkpoint/resume capability.

**Key Class:**
```python
class CuratedCollectionRunner:
    def __init__(self, user_name: str = "sajak")
    async def collect_topic(category, topic_name, config) -> int
    async def collect_surveys() -> int
    async def run_full_collection()
    
    # Checkpoint management
    def _load_checkpoint() -> dict
    def _save_checkpoint()
    def _save_report()
```

**Checkpoint Format:**
```json
{
  "completed_topics": [
    "foundations:Neural Network Fundamentals",
    "foundations:Optimization in Deep Learning"
  ],
  "completed_surveys": [
    "survey deep learning neural network"
  ]
}
```

**Usage:**
```bash
# Show plan without downloading
python run_curated_collection.py --dry-run

# Start fresh collection
python run_curated_collection.py

# Resume from checkpoint
python run_curated_collection.py --resume

# Specific user name
python run_curated_collection.py --user yourname
```

---

## 5. Data Flow

### 5.1 Z-Library Book Download Flow

```
User Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   agent.py      â”‚
â”‚ parse_intent()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  memory.py      â”‚â”€â”€â”€â”€â–ºâ”‚   Supabase      â”‚
â”‚ check_duplicate â”‚     â”‚ (cloud lookup)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (if not duplicate)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ mcp_server.py   â”‚
â”‚ search_zlibrary â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Z-Library     â”‚
â”‚   Website       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ download_book() â”‚
â”‚ â†’ data/books/   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  memory.py      â”‚â”€â”€â”€â”€â–ºâ”‚   Supabase      â”‚
â”‚ add_book()      â”‚     â”‚ (save record)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 arXiv Paper Collection Flow

```
User/Script Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ arxiv_collector.py  â”‚
â”‚ collect_papers()    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    memory.py        â”‚â”€â”€â”€â”€â–ºâ”‚    Supabase     â”‚
â”‚ check_paper_dup()   â”‚     â”‚ (cloud lookup)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ (if not duplicate)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   arXiv API         â”‚
â”‚ export.arxiv.org    â”‚
â”‚ /api/query          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ (XML response)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ _parse_arxiv_resp() â”‚
â”‚ â†’ ArxivPaper list   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ download_pdf()      â”‚
â”‚ â†’ data/papers/      â”‚
â”‚ (rate limited)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    memory.py        â”‚â”€â”€â”€â”€â–ºâ”‚    Supabase     â”‚
â”‚ add_paper()         â”‚     â”‚ (save record)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 Bulk Collection Flow (run_curated_collection.py)

```
Start Script
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load checkpoint      â”‚
â”‚ (or start fresh)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ For each     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ category     â”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
           â”‚                              â”‚
           â–¼                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚ For each     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
    â”‚ topic        â”‚             â”‚        â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚        â”‚
           â”‚                     â”‚        â”‚
           â”‚ (skip if in         â”‚        â”‚
           â”‚  checkpoint)        â”‚        â”‚
           â–¼                     â”‚        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚        â”‚
    â”‚ For each     â”‚â—„â”€â”€â”€â”        â”‚        â”‚
    â”‚ query        â”‚    â”‚        â”‚        â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚        â”‚        â”‚
           â”‚            â”‚        â”‚        â”‚
           â–¼            â”‚        â”‚        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚        â”‚        â”‚
    â”‚ ArxivCollect â”‚    â”‚        â”‚        â”‚
    â”‚ .collect()   â”‚    â”‚        â”‚        â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚        â”‚        â”‚
           â”‚            â”‚        â”‚        â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚        â”‚
                                 â”‚        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚        â”‚
    â”‚ Save topic   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
    â”‚ to checkpointâ”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚ Next categoryâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Collect      â”‚
    â”‚ surveys      â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Save report  â”‚
    â”‚ Print stats  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Configuration

### 6.1 Environment Variables (.env)

```bash
# === REQUIRED ===
OPENAI_API_KEY=sk-...              # For GPT-4o and embeddings

# === SUPABASE (Cloud Memory) ===
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=eyJhbGciOiJIUzI1NiIs...  # anon/public key

# === Z-LIBRARY ===
BOOK_DATA_FOLDER=data/books
RESOURCES_FILE=data/resources.json
MAX_DOWNLOADS_PER_ACCOUNT=9

# === ARXIV ===
ARXIV_DATA_FOLDER=data/papers
ARXIV_RESOURCES_FILE=data/arxiv_resources.json
ARXIV_API_RATE_LIMIT=3.0           # seconds between API calls
ARXIV_PDF_DELAY=1.0                # seconds between PDF downloads
ARXIV_MAX_PER_QUERY=100            # max results per API query
ARXIV_MAX_RETRIES=3
ARXIV_DOWNLOAD_TIMEOUT=120         # seconds

# === AGENT ===
MAX_CONVERSATION_HISTORY=20
```

### 6.2 Z-Library Accounts (accounts.json)

```json
{
    "accounts": [
        {
            "email": "user1@email.com",
            "cookies": {
                "remix_userid": "12345678",
                "remix_userkey": "abcdef123..."
            }
        },
        {
            "email": "user2@email.com",
            "cookies": {
                "remix_userid": "87654321",
                "remix_userkey": "fedcba321..."
            }
        }
    ]
}
```

**How to get cookies:**
1. Log into Z-Library in browser
2. Open DevTools â†’ Application â†’ Cookies
3. Copy `remix_userid` and `remix_userkey` values

---

## 7. Database Schema

### 7.1 Supabase Tables

**book_memory** (Z-Library books):
```sql
CREATE TABLE book_memory (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    normalized_title TEXT NOT NULL,      -- lowercase, no punctuation
    authors TEXT,
    source TEXT,                          -- 'zlibrary'
    search_topic TEXT,                    -- original search query
    embedding FLOAT8[],                   -- 1536-dim vector
    downloaded_by TEXT,                   -- user who downloaded
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_normalized_title ON book_memory(normalized_title);
CREATE INDEX idx_downloaded_by ON book_memory(downloaded_by);
```

**paper_memory** (arXiv papers):
```sql
CREATE TABLE paper_memory (
    id SERIAL PRIMARY KEY,
    arxiv_id TEXT UNIQUE NOT NULL,       -- e.g., "2301.00001"
    title TEXT NOT NULL,
    normalized_title TEXT NOT NULL,
    authors TEXT,
    abstract TEXT,
    categories TEXT,                      -- e.g., "cs.LG, cs.AI"
    embedding FLOAT8[],                   -- 1536-dim vector
    downloaded_by TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_paper_arxiv_id ON paper_memory(arxiv_id);
CREATE INDEX idx_paper_normalized_title ON paper_memory(normalized_title);
CREATE INDEX idx_paper_downloaded_by ON paper_memory(downloaded_by);
```

### 7.2 Local Files

**resources.json** (Book metadata):
```json
{
    "books": [
        {
            "title": "Deep Learning",
            "authors": "Ian Goodfellow",
            "filename": "deep_learning_goodfellow.pdf",
            "topic": "deep learning fundamentals",
            "downloaded_at": "2026-02-04T10:30:00"
        }
    ]
}
```

**arxiv_resources.json** (Paper metadata):
```json
{
    "papers": [
        {
            "arxiv_id": "2301.00001",
            "title": "Attention Is All You Need",
            "authors": ["Vaswani", "Shazeer", "..."],
            "categories": ["cs.CL", "cs.LG"],
            "filename": "2301_00001_Attention_Is_All_You_Need.pdf",
            "downloaded_at": "2026-02-04T10:30:00"
        }
    ]
}
```

---

## 8. API Integrations

### 8.1 arXiv API

**Base URL:** `http://export.arxiv.org/api/query`

**Query Parameters:**
| Parameter | Description | Example |
|-----------|-------------|---------|
| `search_query` | Search terms | `all:deep AND all:learning` |
| `start` | Pagination offset | `0` |
| `max_results` | Results per page | `100` (max 2000) |
| `sortBy` | Sort field | `relevance`, `submittedDate` |
| `sortOrder` | Sort direction | `ascending`, `descending` |

**Example Request:**
```
http://export.arxiv.org/api/query?search_query=(all:transformer AND all:attention) AND (cat:cs.LG OR cat:cs.CL)&start=0&max_results=10&sortBy=relevance
```

**Response:** Atom XML feed

**Rate Limits:**
- 1 request per 3 seconds (enforced in code)
- Be polite or get IP banned!

### 8.2 OpenAI API

**Used for:**
1. GPT-4o: Agent conversations and tool calls
2. text-embedding-3-small: Duplicate detection embeddings

**Models:**
| Purpose | Model | Cost |
|---------|-------|------|
| Agent Chat | gpt-4o | ~$5/1M tokens |
| Embeddings | text-embedding-3-small | ~$0.02/1M tokens |

### 8.3 Supabase REST API

**Authentication:**
```python
headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}"
}
```

**PostgREST Queries:**
```python
# Insert
client.from_("paper_memory").insert({...}).execute()

# Select with filter
client.from_("paper_memory").select("*").eq("arxiv_id", "2301.00001").execute()

# Select all
client.from_("book_memory").select("*").execute()
```

---

## 9. Running the System

### 9.1 Initial Setup

```bash
# 1. Create virtual environment
python -m venv LLMenv
.\LLMenv\Scripts\Activate.ps1  # Windows

# 2. Install dependencies
pip install -r requirements.txt

# 3. Copy environment template
copy .env.example .env

# 4. Fill in .env with your keys

# 5. Create Supabase tables (run SQL from setup_supabase.py)
python setup_supabase.py
```

### 9.2 Interactive Mode (Agent Chat)

```bash
# Start the agent
python run.py

# Or directly
python agent.py
```

**Example session:**
```
Select data source:
1. Z-Library (Books)
2. arXiv (Research Papers)
Enter choice (1/2): 2

You: Download 100 papers on transformers and attention mechanisms

RAMESH: Namaste! Let me collect those papers for you...
```

### 9.3 Bulk Collection Mode

```bash
# See the collection plan
python run_curated_collection.py --dry-run

# Start collection
python run_curated_collection.py

# Resume after interruption
python run_curated_collection.py --resume
```

### 9.4 Common Commands

```bash
# Check downloaded paper count
Get-ChildItem data/papers/*.pdf | Measure-Object

# Check downloaded book count
Get-ChildItem data/books/*.pdf | Measure-Object

# View collection report
cat collection_report.json

# Clear checkpoint (start fresh)
Remove-Item collection_checkpoint.json
```

---

## 10. Troubleshooting

### Issue: SSL Certificate Error on Windows

**Error:**
```
SSLCertVerificationError: certificate verify failed: unable to get local issuer certificate
```

**Solution:**
```bash
pip install certifi
```

The code automatically uses certifi for SSL context.

### Issue: arXiv Returns 0 Papers

**Cause:** Exact phrase matching is too strict.

**Solution:** The code now uses word-by-word AND matching:
```python
# Instead of: all:"deep learning transformers"
# Now uses:   (all:deep AND all:learning AND all:transformers)
```

### Issue: Supabase Connection Failed

**Check:**
1. `.env` has correct `SUPABASE_URL` and `SUPABASE_KEY`
2. Tables exist (run `setup_supabase.py`)
3. RLS policies allow inserts

### Issue: Z-Library Download Limit Reached

**Solution:** Add more accounts to `accounts.json`. The system auto-rotates.

### Issue: Collection Stopped Mid-Way

**Solution:** Use `--resume` flag:
```bash
python run_curated_collection.py --resume
```

Progress is saved in `collection_checkpoint.json`.

### Issue: Too Many Duplicate Papers

**Check:**
- Supabase `paper_memory` table has embeddings
- `SIMILARITY_THRESHOLD` in memory.py (default: 0.85)

---

## Quick Reference

### Key Commands

| Action | Command |
|--------|---------|
| Start agent | `python run.py` |
| Bulk collect | `python run_curated_collection.py` |
| Resume collection | `python run_curated_collection.py --resume` |
| Dry run | `python run_curated_collection.py --dry-run` |
| Check papers | `Get-ChildItem data/papers/*.pdf \| Measure-Object` |
| View report | `cat collection_report.json` |

### File Locations

| Data | Location |
|------|----------|
| Books PDFs | `data/books/` |
| Paper PDFs | `data/papers/` |
| Book metadata | `data/resources.json` |
| Paper metadata | `data/arxiv_resources.json` |
| Checkpoint | `collection_checkpoint.json` |
| Report | `collection_report.json` |

### Important Limits

| Limit | Value |
|-------|-------|
| arXiv API rate | 3 seconds between requests |
| arXiv max per query | 2000 results |
| Z-Library per account | 9 downloads |
| Embedding similarity threshold | 85% |

---

**Created by Sajak ğŸ‡³ğŸ‡µ**  
*"Ramesh never forgets, and now your whole team won't either!"*
